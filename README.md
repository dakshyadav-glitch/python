# ETL Pipeline â€“ JSONPlaceholder Users API

## ğŸ“Œ Project Overview

This project implements a **production-style ETL (Extract, Transform, Load) pipeline** using Python. The pipeline extracts user data from a public API, cleans and validates it, stores it in a relational database (SQLite), and generates meaningful business insights using SQL.

The project is intentionally designed to handle **real-world data engineering concerns** such as unreliable APIs, nested JSON transformation, data validation, logging, and schema-aware analytics.

---

## ğŸ—ï¸ Overall Architecture

```
API (JSONPlaceholder)
        â†“
Extract (Python + Requests)
        â†“
Transform & Clean (Pandas)
        â†“
Validate Data
        â†“
Save CSV
        â†“
Load into SQLite
        â†“
Run SQL Business Insights
```

---

## ğŸ”— Data Source

* **API**: [https://jsonplaceholder.typicode.com/users](https://jsonplaceholder.typicode.com/users)
* **Format**: Nested JSON
* **Records**: 10 users

---

## ğŸ“‚ Project Structure

```
etl_pipeline/
â”‚
â”œâ”€â”€ main.py            # Pipeline orchestrator
â”œâ”€â”€ extract.py         # API extraction with retry & timeout handling
â”œâ”€â”€ transform.py       # JSON flattening & transformation
â”œâ”€â”€ validate.py        # Data quality rules
â”œâ”€â”€ load.py            # CSV export & SQLite insertion
â”œâ”€â”€ insights.py        # SQL-based business insights
â”œâ”€â”€ logger.py          # Centralized logging configuration
â”œâ”€â”€ users.db           # SQLite database (auto-created)
â”œâ”€â”€ users_clean.csv    # Cleaned dataset (auto-generated)
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used

* **Python 3**
* **Requests** â€“ API calls
* **Pandas** â€“ Data transformation
* **SQLite** â€“ Relational database
* **SQL** â€“ Business insights
* **Logging** â€“ Pipeline observability

---

## âœ… Data Validation Rules

| Rule                | Action |
| ------------------- | ------ |
| Duplicate `user_id` | Reject |
| Email without `@`   | Reject |
| City is NULL        | Reject |
| Zipcode length < 5  | Reject |

Invalid records are **excluded** from downstream processing to maintain data quality.

---

## ğŸ”„ Transformation Highlights

* Flattens nested JSON fields (`address`, `geo`, `company`)
* Standardizes column naming
* Extracts latitude & longitude for geo insights
* Produces a structured tabular dataset

---

## ğŸ—„ï¸ Database Schema (`users` table)

| Column       | Description            |
| ------------ | ---------------------- |
| user_id      | Unique user identifier |
| name         | User full name         |
| username     | Username               |
| email        | Email address          |
| city         | City name              |
| zipcode      | Postal code            |
| phone_no     | Phone number           |
| website      | Website                |
| company_name | Company name           |
| latitude     | Geographic latitude    |
| longitude    | Geographic longitude   |

---

## ğŸ“Š Business Insights Generated

The pipeline generates insights using **pure SQL**, including:

1. Total valid users
2. Users per company
3. Users per city
4. Users per city per company
5. Phone number coverage
6. Email vs website domain mismatch
7. Hemisphere distribution (Northern vs Southern)
8. Email domain classification (Work vs Personal)
9. Website domain distribution (.com, .org, .net, etc.)

These insights are designed to handle **high-cardinality datasets** where naive aggregation would be misleading.

---

## ğŸ§  Key Design Decisions

* **Fail-fast extraction**: Pipeline stops if API extraction fails
* **Retry & timeout handling** for unreliable APIs
* **Schema-aware analytics** to avoid invalid assumptions
* **Pattern-based insights** instead of fake aggregation

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Create virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install requests pandas
```

### 3ï¸âƒ£ Run the pipeline

```bash
python main.py
```

SQLite database and CSV will be generated automatically.

---

## ğŸ§ª Example Use Cases

* ETL pipeline demonstration
* Data engineering portfolio project
* SQL analytics practice
* API-to-database ingestion

---

## ğŸ“Œ Notes

* This project uses a **public demo API**; data values are synthetic
* Designed for **clarity, correctness, and interview readiness**

---

## ğŸ‘¤ Author

Daksh Yadav
*Data Engineering / ETL Pipeline Project*

---


